## Real Time Data Persistence and API Development
### ğŸ¯ Objective
Design and implement a robust **real-time data persistence pipeline** using:
- **Kafka** â†’ for data ingestion  
- **Azure SQL database (MSSQL)** â†’ for data storage  
- **Python microservices (FastAPI)** â†’ for API development and data retrieval

**Architecture Summary:** Kafka â†’ Consumer (validation) â†’ Azure SQL â†’ FastAPI API â†’ Client
____________________________________________________________________________________________________________________________________________________________________________________________________________
### ğŸ§° Technologies Used
- ğŸ **Python 3.10+**
- â˜ï¸ **Confluent Kafka Cloud**
- ğŸ—„ï¸ **Azure SQL Database (MSSQL)**
- âš¡ **FastAPI**
- ğŸ³ **Docker**
- â˜¸ï¸ **Kubernetes**
____________________________________________________________________________________________________________________________________________________________________________________________________________
### ğŸ“‚ Folder Structure
```
real_time_pipeline/
â”œâ”€â”€ sql/
â”‚ â””â”€â”€ schema.sql
â”‚
â”œâ”€â”€ kafka_consumer/
â”‚ â”œâ”€â”€ consumer.py
â”‚ â”œâ”€â”€ producer_test.py
â”‚ â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ fastapi_api/
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ k8s/
â”‚ â”œâ”€â”€ kafka_consumer_deployment.yaml
â”‚ â”œâ”€â”€ fastapi_api_deployment.yaml
â”‚ â”œâ”€â”€ services.yaml
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```
____________________________________________________________________________________________________________________________________________________________________________________________________________
### ğŸ§© Task 1: Designing the Database (Azure SQL)
The relational schema is designed from the incoming transaction JSON and follows normalization.
**Tables:**
1.	Transactions
2.	Merchants
3.	Payers
- Each transaction references a merchant and payer using foreign keys.

**Schema file:**
-	sql/schema.sql â€“ contains table creation and indexing scripts.
Features:
-	Normalized relational model with primary/foreign keys
-	Indexed for optimized lookups by ```transaction_id```, ```timestamp```, and ```merchant_id```
____________________________________________________________________________________________________________________________________________________________________________________________________________
### ğŸ§© Task 2: Building the Real-Time Kafka Pipeline
**Components**

**1. Kafka Producer (```producer_test.py```)**
- Generates and sends both valid and invalid transaction messages concurrently.
- Publishes valid and invalid JSON messages to Kafka topic ```transactions```.

**2. Kafka Consumer (```consumer.py```)**
- Consumes JSON messages from the ```transactions``` topic.
- Validates each message using **Pydantic schema models**.
- Persists valid records into **Azure SQL**.
- Sends invalid records to a dedicated Kafka error topic ```transactions_error```.

Both scripts are containerized and can run independently.
**Flow**
```
Producer â†’ Kafka Topic â†’ Consumer â†’ Azure SQL
                       â†³ Invalid â†’ Error Topic
```
____________________________________________________________________________________________________________________________________________________________________________________________________________
### ğŸ§© Task 3: Building the FastAPI Layer
To make the stored data accessible, a lightweight **FastAPI** service provides two REST APIs:

**1. Fetch Transaction Details**
```
GET /transactions/{transaction_id}
```
- Returns transaction details including merchant and payer info.

**2. Top Merchants by Transaction Volume**
```
GET /merchants/top?start_date=<YYYY-MM-DD>&end_date=<YYYY-MM-DD>
```
- Returns top 5 merchants ranked by total transaction amount within a date range.
- Each endpoint connects directly to the Azure SQL database using **pyodbc**.
____________________________________________________________________________________________________________________________________________________________________________________________________________
### Environment Configuration
All credentials are managed via ```.env``` file (excluded from repo for security).
An example template is provided in : ```.env.example```:

| Variable | Description |
|-----------|-------------|
| `KAFKA_BOOTSTRAP` | Kafka broker connection string |
| `KAFKA_USERNAME` | Kafka confluent api key |
| `KAFKA_PASSWORD` | Kafka confluent api secret |
| `SQL_SERVER` | Azure SQL server name |
| `SQL_DB` | Database name |
| `SQL_USER` | Database username |
| `SQL_PASSWORD` | Database password |

- None of these values are hardcoded or committed to GitHub.
____________________________________________________________________________________________________________________________________________________________________________________________________________
### ğŸ§ª Testing & Validation
#### Kafka Producer Test :
Run producer_test.py to generate valid and invalid transaction messages:
```
python kafka_consumer/producer_test.py
```
-	Valid messages â†’ Sent to Kafka topic ````transactions```
-	Invalid messages â†’ Missing critical fields (eg. transaction_id), used to test validation logic
#### Kafka Consumer Test :
Run consumer.py to process incoming Kafka messages and persist them in Azure SQL:
```
python kafka_consumer/consumer.py
```
-	Valid messages â†’ Validated via Pydantic models and stored in Azure SQL tables 
-	Invalid messages â†’ Captured and re-published to Kafka topic ```transactions_error```
#### API Testing :
**Start the FastAPI Server**
```
cd fastapi_api
uvicorn main:app --reload
```
Access the interactive docs at :
```http://127.0.0.1:8000/docs```
**Get Transaction details:**
```GET /transactions/TX-10001```
- Returns transaction info along with merchant and payer details

**Top 5 Merchants by transaction amount:**
```GET /merchants/top?start_date=2025-08-20&end_date=2025-08-25```
- Returns the top 5 merchants ranked by total transaction volume within the specified date range
#### Azure SQL Verification:
Checked via Azure Query Editor (Preview) verified records insertion in:
- Transaction
- Merchants
- Payers
____________________________________________________________________________________________________________________________________________________________________________________________________________
### â˜¸ï¸ Containerization and Deployment
These manifests define the deployment and service configuration for both Kafka Consumer and FastAPI API pods on AKS.
**Resources :**
- **kafka_consumer_deployment.yaml** â†’ Kafka consumer pod
- **fastapi_api_deployment.yaml** â†’ FastAPI API pod
- **services.yaml** â†’ Defines cluster-accessible endpoints

**Deployment steps**
1. Build Docker Images
```
docker build -t <your_docker_repo>/kafka_consumer:latest ./kafka_consumer
docker build -t <your_docker_repo>/fastapi_api:latest ./fastapi_api
```
2. Push to container registry
```
docker push <your_docker_repo>/kafka_consumer:latest
docker push <your_docker_repo>/fastapi_api:latest
```
3. Create Kubernetes secrets for credentials.
```
kubectl create secret generic mssql-secrets \
  --from-literal=server=<SQL_SERVER> \
  --from-literal=db=<SQL_DB> \
  --from-literal=user=<SQL_USER> \
  --from-literal=password=<SQL_PASSWORD>

kubectl create secret generic kafka-secrets \
  --from-literal=bootstrap=<KAFKA_BOOTSTRAP>
```
4. Deploy all resources
```
kubectl apply -f k8s/kafka_consumer_deployment.yaml
kubectl apply -f k8s/fastapi_api_deployment.yaml
kubectl apply -f k8s/services.yaml
```
____________________________________________________________________________________________________________________________________________________________________________________________________________
#### Author : 
Name: Ankita Tripathi

Email: ankitatripathivns@gmail.com

Date: January 2026

